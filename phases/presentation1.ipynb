{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ef44845",
   "metadata": {},
   "source": [
    "<div style=\"background:#f5f5f7; border:2px solid #d0d0d5; padding:20px; border-radius:12px;\">\n",
    "\n",
    "# **Naive GNN Approach — Beginner-Friendly Explanation**\n",
    "\n",
    "### <span style=\"color:#4a4a4f;\">Understanding how neural networks can work on graph data</span>\n",
    "\n",
    "Graphs are special because they are made of **three different types of information**:\n",
    "\n",
    "* **Nodes** (things)\n",
    "* **Edges** (relationships between things)\n",
    "* **Global information** (a single vector describing the entire graph)\n",
    "\n",
    "In normal neural networks, data usually has a fixed shape (like an image grid or a sequence). But graphs are irregular — each graph can have a different number of nodes and edges. So we need to think differently.\n",
    "\n",
    "This part of your instructor’s presentation is showing the **very first**, **simplest possible**, **connectivity-free** graph neural network.\n",
    "\n",
    "---\n",
    "\n",
    "## **What is the Naive Approach?**\n",
    "\n",
    "This approach tries to answer one simple question:\n",
    "\n",
    "> *If I have vectors for nodes, edges, and the whole graph, can I learn better versions of those vectors using neural networks?*\n",
    "\n",
    "The answer is **yes**, and we do this using **MLPs** (Multilayer Perceptrons).\n",
    "\n",
    "But here is the key idea:\n",
    "\n",
    "<div style=\"background:#ffffff; border-left: 4px solid #6a6a75; padding:10px;\">\n",
    "Each node, each edge, and the global graph vector is processed **independently**, with **no use of graph connectivity**.\n",
    "</div>\n",
    "\n",
    "This is why this method is called **naive**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step-by-Step Explanation (No Leaps)**\n",
    "\n",
    "### **1. Every node has a feature vector**\n",
    "\n",
    "For example:\n",
    "\n",
    "* A molecule atom may have features like its type, electric charge, etc.\n",
    "* A social network user may have features like age or interests.\n",
    "\n",
    "Let’s call the feature of a node:\n",
    "\n",
    "x_v\n",
    "\n",
    "### **2. Every edge has a feature vector**\n",
    "\n",
    "Edges connect two nodes, but in the naive model, we ignore that connection.\n",
    "\n",
    "Let an edge feature be:\n",
    "\n",
    "x_e\n",
    "\n",
    "### **3. The whole graph has a single global vector**\n",
    "\n",
    "This is optional but useful.\n",
    "\n",
    "Let it be:\n",
    "\n",
    "u\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Apply a separate MLP to each type of feature**\n",
    "\n",
    "We use three different neural networks:\n",
    "\n",
    "* **MLP_V** → updates each node\n",
    "* **MLP_E** → updates each edge\n",
    "* **MLP_U** → updates the global feature\n",
    "\n",
    "So we compute:\n",
    "\n",
    "new_node_feature = MLP_V(old_node_feature)\n",
    "new_edge_feature = MLP_E(old_edge_feature)\n",
    "new_global_feature = MLP_U(old_global_feature)\n",
    "\n",
    "Each node gets updated **independently**, each edge gets updated **independently**, and the global graph feature gets updated **independently**.\n",
    "\n",
    "No messages. No neighbors. No structure.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why this is called a “GNN Layer”?**\n",
    "\n",
    "Because the input is a graph (in the form of its attributes), and the output is *also* a graph with updated attributes.\n",
    "\n",
    "Technically, it **does** operate on graph data, even though it ignores connections.\n",
    "\n",
    "Also, as with normal neural networks, you can **stack several layers** of this.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why teach this naive approach first?**\n",
    "\n",
    "Because it answers the most basic question:\n",
    "\n",
    "<div style=\"background:#ffffff; border-left: 4px solid #6a6a75; padding:10px;\">\n",
    "<b>How do we even begin to apply neural networks to graphs?</b>\n",
    "</div>\n",
    "\n",
    "Before introducing real GNNs (which use neighbors and message passing), your instructor wants you to understand the simplest functional building block.\n",
    "\n",
    "---\n",
    "\n",
    "## **Limitations (Why this is not enough)**\n",
    "\n",
    "This simple approach **cannot**:\n",
    "\n",
    "* Learn from graph connectivity\n",
    "* Detect patterns like triangles or chains\n",
    "* Use which nodes are neighbors\n",
    "* Distinguish graphs with the same node features but different shapes\n",
    "\n",
    "In other words:\n",
    "\n",
    "> *This approach learns from features, not structure.*\n",
    "\n",
    "This is why the next stage of the course will introduce **message passing**, which *does* use the graph structure.\n",
    "\n",
    "---\n",
    "\n",
    "## **Visual Summary (from the slide)**\n",
    "\n",
    "* Each attribute type (Nodes, Edges, Global) is fed into its own MLP.\n",
    "* The output is a new graph whose attributes are updated.\n",
    "* Structure is ignored.\n",
    "* This is the simplest possible GNN layer.\n",
    "\n",
    "---\n",
    "\n",
    "# **End of Explanation**\n",
    "\n",
    "If you want, I can also prepare separate markdown cells for:\n",
    "\n",
    "* A comparison with real message-passing GNNs\n",
    "* A diagram in ASCII art\n",
    "* A second markdown cell for the next slide\n",
    "* A compact summary box for your PDF\n",
    "\n",
    "Just tell me what you want next.\n",
    "\n",
    "## **Step-by-step — visual, intuitive example**\n",
    "\n",
    "<div style=\"background:#ffffff; border-left:4px solid #3a3a40; padding:12px; border-radius:8px;\">\n",
    "\n",
    "**Graph layout (picture it left-to-right):**\n",
    "\n",
    "A — B — C\n",
    "\n",
    "* Nodes: **A**, **B**, **C**\n",
    "* Edges: **e1** connects A–B, **e2** connects B–C\n",
    "* Global vector: **G** (one small vector describing whole graph)\n",
    "\n",
    "**Initial (simple) features — imagine these as short lists of numbers):**\n",
    "\n",
    "* A: [1, 0]\n",
    "* B: [0, 1]\n",
    "* C: [1, 1]\n",
    "* e1 (A–B): [0.5]\n",
    "* e2 (B–C): [0.2]\n",
    "* Global G: [0]\n",
    "\n",
    "> These are tiny example feature vectors so you can *see* the numbers transform. They are illustrative — not outputs of a real MLP.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1 — look at a single layer of the naive GNN**\n",
    "\n",
    "* The model contains three separate small MLPs: **MLP_V** (for nodes), **MLP_E** (for edges), **MLP_U** (for global).\n",
    "* Importantly: **each node is processed alone** — it does not see its neighbours.\n",
    "\n",
    "**Apply the node-MLP to each node individually:**\n",
    "\n",
    "* A_new = MLP_V(A) → imagine it maps [1,0] → [0.9, 0.1]\n",
    "* B_new = MLP_V(B) → [0,1] → [0.2, 0.8]\n",
    "* C_new = MLP_V(C) → [1,1] → [0.6, 0.7]\n",
    "\n",
    "**Apply the edge-MLP to each edge individually:**\n",
    "\n",
    "* e1_new = MLP_E(e1) → [0.5] → [0.45]\n",
    "* e2_new = MLP_E(e2) → [0.2] → [0.18]\n",
    "\n",
    "**Apply the global-MLP to the global vector:**\n",
    "\n",
    "* G_new = MLP_U(G) → [0] → [0.05]\n",
    "\n",
    "> Notice: None of the node outputs used A’s neighbors' values. A_new was produced only from A.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2 — imagine stacking another identical layer**\n",
    "\n",
    "We feed the updated features back into the same type of layer (this is stacking).\n",
    "\n",
    "* A_second = MLP_V(A_new) → [0.9,0.1] → [0.85, 0.15]\n",
    "* B_second = MLP_V(B_new) → [0.2,0.8] → [0.12, 0.88]\n",
    "* C_second = MLP_V(C_new) → [0.6,0.7] → [0.55, 0.75]\n",
    "\n",
    "Edges and global updated similarly.\n",
    "\n",
    "**Key mental image:** stacking layers is like running the same personal grooming routine on each person, then running it again; every person gets better at looking a bit different, but nobody has talked with anyone else.\n",
    "\n",
    "---\n",
    "\n",
    "### **What you should *see* in your mind**\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<center><img src=\"GNN_bootcamp-main/images/graph_attributes.png\" width=\"500\" alt=\"Graph Attributes\"></center>\n",
    "<center><img src=\"GNN_bootcamp-main/images/naive_1.png\" width=\"500\" alt=\"Naive GNN Architecture\"></center>\n",
    "\n",
    "<small>A single layer of a simple GNN. A graph serves as the input, and each component (V, E, U) gets updated by an MLP to produce a new graph. Each function subscript indicates a separate function for a different graph attribute at the n-th layer of the GNN model.\n",
    "This image illustrates a graph-independent GNN layer, where the global feature \\(U_n\\), node features \\(V_n\\), and edge features \\(E_n\\) are each passed through their own update functions \\(f_U\\), \\(f_V\\), and \\(f_E\\). These functions independently transform the features to produce \\(U_{n+1}\\), \\(V_{n+1}\\), and \\(E_{n+1}\\), without using the graph’s connectivity.\n",
    "\n",
    "</small>\n",
    "\n",
    "<i><center><small>Images from <a href=\"https://distill.pub/2021/gnn-intro/\">Distill</a></small></center></i>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960efa97",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#faf9fc; padding:22px; border-radius:14px; border:1px solid #d7d7e8; color:#222; font-family:'Segoe UI', sans-serif; line-height:1.55;\">\n",
    "\n",
    "# **Cora Dataset — Overview**\n",
    "\n",
    "The Cora dataset is a citation graph of 2,708 machine-learning research papers, divided into seven categories.  \n",
    "Each paper is turned into a binary vector over 1,433 unique words, showing which words appear in that document.\n",
    "\n",
    "The graph has 5,429 directed edges — each one means “paper A cites paper B.”\n",
    "\n",
    "The dataset comes as two files:\n",
    "\n",
    "* a **content** file holding each paper’s word vector and its class label,\n",
    "* and a **citations** file listing all directed citation links between papers.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4910f34",
   "metadata": {},
   "source": [
    "<center><img src=\"GNN_bootcamp-main/images/cora.png\" width=600></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "489d0c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nodes: 2708\n",
      "number of features: 1433\n",
      "number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "import torch\n",
    "\n",
    "# Load the CORA dataset\n",
    "dataset = Planetoid(root=\".\", name=\"Cora\")\n",
    "# Access the first graph object\n",
    "data = dataset[0]\n",
    "print(f\"number of nodes: {data.x.shape[0]}\")\n",
    "print(f\"number of features: {data.x.shape[1]}\")\n",
    "print(f\"number of classes: {torch.unique(data.y).size()[0]}\")\n",
    "\n",
    "# The data object contains train_mask, val_mask, and test_mask\n",
    "train_mask = data.train_mask # this is simply about the getting train data \n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e4e0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 1.9194, Train Accuracy: 0.1714, Test Accuracy: 0.3320\n",
      "Epoch: 10, Loss: 1.5917, Train Accuracy: 0.9929, Test Accuracy: 0.4630\n",
      "Epoch: 20, Loss: 1.4418, Train Accuracy: 1.0000, Test Accuracy: 0.4900\n",
      "Epoch: 30, Loss: 1.4467, Train Accuracy: 1.0000, Test Accuracy: 0.5030\n",
      "Epoch: 40, Loss: 1.4761, Train Accuracy: 1.0000, Test Accuracy: 0.5010\n",
      "Epoch: 50, Loss: 1.4815, Train Accuracy: 1.0000, Test Accuracy: 0.5030\n",
      "Epoch: 60, Loss: 1.4600, Train Accuracy: 1.0000, Test Accuracy: 0.5120\n",
      "Epoch: 70, Loss: 1.4258, Train Accuracy: 1.0000, Test Accuracy: 0.5170\n",
      "Epoch: 80, Loss: 1.3980, Train Accuracy: 1.0000, Test Accuracy: 0.5190\n",
      "Epoch: 90, Loss: 1.3827, Train Accuracy: 1.0000, Test Accuracy: 0.5140\n",
      "Epoch: 100, Loss: 1.3717, Train Accuracy: 1.0000, Test Accuracy: 0.5200\n",
      "Epoch: 110, Loss: 1.3612, Train Accuracy: 1.0000, Test Accuracy: 0.5190\n",
      "Epoch: 120, Loss: 1.3547, Train Accuracy: 1.0000, Test Accuracy: 0.5190\n",
      "Epoch: 130, Loss: 1.3491, Train Accuracy: 1.0000, Test Accuracy: 0.5260\n",
      "Epoch: 140, Loss: 1.3449, Train Accuracy: 1.0000, Test Accuracy: 0.5300\n",
      "Epoch: 150, Loss: 1.3410, Train Accuracy: 1.0000, Test Accuracy: 0.5320\n",
      "Epoch: 160, Loss: 1.3372, Train Accuracy: 1.0000, Test Accuracy: 0.5330\n",
      "Epoch: 170, Loss: 1.3337, Train Accuracy: 1.0000, Test Accuracy: 0.5360\n",
      "Epoch: 180, Loss: 1.3305, Train Accuracy: 1.0000, Test Accuracy: 0.5390\n",
      "Epoch: 190, Loss: 1.3275, Train Accuracy: 1.0000, Test Accuracy: 0.5430\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim_in, dim_h, dim_out):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(dim_in, dim_h)\n",
    "        self.linear2 = nn.Linear(dim_h, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "mlp = MLP(dataset.num_features, 16, dataset.num_classes)\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "train_data = TensorDataset(torch.as_tensor(data.x[train_mask,:]),torch.as_tensor(data.y[train_mask]))\n",
    "test_data = TensorDataset(torch.as_tensor(data.x[test_mask,:]),torch.as_tensor(data.y[test_mask]))\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "    return torch.sum(y_pred == y_true) / len(y_true)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    prediction = mlp(train_data.tensors[0])\n",
    "    loss = loss_fn(prediction,train_data.tensors[1])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_acc = accuracy(torch.argmax(prediction, dim=1),train_data.tensors[1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = mlp(test_data.tensors[0])\n",
    "        loss = loss_fn(prediction,test_data.tensors[1])\n",
    "        test_acc = accuracy(torch.argmax(prediction, dim=1),test_data.tensors[1])\n",
    "    if epoch%10==0:\n",
    "        print(f'Epoch: {epoch}, Loss: {loss.item():.4f}, Train Accuracy: {train_acc:.4f}, Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e15f5bc",
   "metadata": {},
   "source": [
    "\n",
    "It is a **2-layer MLP classifier**:\n",
    "\n",
    "```\n",
    "\n",
    "input → Linear → ReLU → Linear → softmax (inside CrossEntropy)\n",
    "\n",
    "````\n",
    "\n",
    "So yes, this is **exactly a standard NN**.\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Your accuracy is low because an MLP *ignores the graph structure*.**\n",
    "\n",
    "This is the **main reason**.\n",
    "\n",
    "Cora classification depends heavily on **propagating information through the graph**.  \n",
    "Papers in the same class tend to cite each other → so GNNs use edges to share features.\n",
    "\n",
    "Your MLP sees only:\n",
    "\n",
    "- each node's bag-of-words vector (x),\n",
    "- but **not its neighbors**,  \n",
    "- not its citation edges,  \n",
    "- not the graph topology.\n",
    "\n",
    "So it behaves like a **bag-of-words text classifier trained on 140 nodes** (the Cora train split).\n",
    "\n",
    "That is **too little data**, so you get:\n",
    "\n",
    "✔️ **Training accuracy = 100%** (overfitting)  \n",
    "✔️ **Test accuracy ≈ 0.55** (no generalization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b73cb84",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#faf9fc; padding:22px; border-radius:14px; border:1px solid #d7d7e8; color:#222; font-family:'Segoe UI', sans-serif; line-height:1.55;\">\n",
    "\n",
    "# **DeepWalk — simple, intuitive explanation**\n",
    "\n",
    "*DeepWalk* is a method that learns vector representations for nodes in a graph using ideas borrowed from language modeling.  \n",
    "It basically treats walks on a graph the same way word sequences are treated in text, and uses that to learn useful embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "## **How DeepWalk works (two main steps)**\n",
    "\n",
    "### **1. Random Walks**\n",
    "\n",
    "DeepWalk starts by exploring the graph through random walks:\n",
    "\n",
    "- For each node, it generates a fixed number \\(k\\) of random paths.\n",
    "- Each path has a fixed length \\(l\\).\n",
    "- These sequences of nodes act like “sentences.”\n",
    "\n",
    "The idea is simple:  \n",
    "nodes that show up near each other in many of these walks should have similar embeddings, because they live in similar neighborhoods in the graph.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. SkipGram**\n",
    "\n",
    "After generating random walks, DeepWalk uses the **SkipGram** model (from the original word2vec paper by Mikolov et al.) to learn embeddings.\n",
    "\n",
    "SkipGram does this by:\n",
    "\n",
    "- Taking a sequence (like a sentence),\n",
    "- Looking at windows of nearby items,\n",
    "- And making items that appear in the same context end up with similar embeddings.\n",
    "\n",
    "In text, this means similar words have similar vectors.  \n",
    "In DeepWalk, it means nodes that appear close together in random walks become similar in vector space.\n",
    "\n",
    "A good intuitive introduction to SkipGram is here:  \n",
    "https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "\n",
    "---\n",
    "\n",
    "## **How SkipGram is applied to graphs**\n",
    "\n",
    "DeepWalk treats each random walk like a sentence:\n",
    "\n",
    "- Each walk is a sequence of nodes.\n",
    "- The “context window” works the same way: nodes that appear close together in the walk are considered related.\n",
    "- Embeddings start as random vectors.\n",
    "- Using gradient descent, the model adjusts them so that nodes appearing together get closer in the embedding space.\n",
    "\n",
    "Essentially, the neural network learns to place similar nodes near each other.\n",
    "\n",
    "---\n",
    "\n",
    "<center><img src=\"GNN_bootcamp-main/images/deepwalk.jpg\" width=400></center>\n",
    "<center><small>image from https://www.geeksforgeeks.org/deepwalk-algorithm/</small></center>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f5006",
   "metadata": {},
   "source": [
    "Example of DeepWalk in Python:\n",
    "\n",
    "To use DeepWalk for node classification on the Cora dataset, we'll follow these steps:\n",
    "\n",
    "- Load the Cora dataset.\n",
    "- Generate random walks to capture the local structure of the graph.\n",
    "- Learn node embeddings using the SkipGram model.\n",
    "- Train a classifier (e.g., logistic regression) using the learned embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d4b6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_walks(G, num_walks, walk_length):\n",
    "    walks = []\n",
    "    nodes = list(G.nodes())\n",
    "    for _ in range(num_walks):\n",
    "        random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walk = [node]\n",
    "            while len(walk) < walk_length:\n",
    "                cur = walk[-1]\n",
    "                neighbors = list(G.neighbors(cur))\n",
    "                if neighbors:\n",
    "                    walk.append(random.choice(neighbors))\n",
    "                else:\n",
    "                    break\n",
    "            walks.append(walk)\n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ec7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nodevectors import Node2Vec\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Load Cora dataset\n",
    "dataset = Planetoid(root=\".\", name=\"Cora\")\n",
    "data = dataset[0]\n",
    "\n",
    "# The data object contains train_mask, val_mask, and test_mask\n",
    "train_mask = data.train_mask\n",
    "val_mask = data.val_mask\n",
    "test_mask = data.test_mask\n",
    "\n",
    "# Convert to NetworkX graph for random walk generation\n",
    "\n",
    "# The starting vertex is each node in the graph, one-by-one, after a random shuffle.\n",
    "\n",
    "G = torch_geometric.utils.to_networkx(data, to_undirected=True)\n",
    "# Parameters\n",
    "num_walks = 10\n",
    "walk_length = 80\n",
    "# Generate random walks\n",
    "walks = generate_random_walks(G, num_walks, walk_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of DeepWalk is: 0.7020\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Convert walks to strings for gensim\n",
    "walks_str = [[str(node) for node in walk] for walk in walks]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(walks_str, vector_size=128, window=10, min_count=0, sg=1, workers=4, epochs=10)\n",
    "\n",
    "# Extract embeddings\n",
    "node_embeddings = {int(node): model.wv[str(node)] for node in G.nodes()}\n",
    "\n",
    "# Prepare data for classification\n",
    "X_train = np.array([node_embeddings[node.item()] for node in torch.where(train_mask)[0]])\n",
    "y_train = np.array(data.y[train_mask])\n",
    "\n",
    "# Train logistic regression classifier\n",
    "classifier = LogisticRegression(max_iter=1000)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "X_test = np.array([node_embeddings[node.item()] for node in torch.where(test_mask)[0]])\n",
    "y_test = np.array(data.y[test_mask])\n",
    "y_pred = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test Accuracy of DeepWalk is: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c8b6b4",
   "metadata": {},
   "source": [
    "<div style=\"background:#f7f7f7; padding:18px; border-radius:10px;\">\n",
    "\n",
    "## Vanilla Neural Network\n",
    "\n",
    "A vanilla neural network processes each node **independently**, using only its feature vector.  \n",
    "It has **no access to edges or connections**, so it cannot use graph structure.\n",
    "\n",
    "---\n",
    "\n",
    "## Graph Structure\n",
    "\n",
    "Graphs have nodes and edges, and the **pattern of connections itself carries information**  \n",
    "(e.g., proximity, communities, structural roles).  \n",
    "A vanilla NN cannot see any of this.\n",
    "\n",
    "---\n",
    "\n",
    "## DeepWalk\n",
    "\n",
    "DeepWalk uses **only graph connectivity**.  \n",
    "It performs random walks (A → B → C → …) and learns embeddings like word2vec.  \n",
    "Nodes that frequently appear together in walks get similar vectors.  \n",
    "It **ignores node features** entirely.\n",
    "\n",
    "---\n",
    "\n",
    "## Vanilla NN vs. DeepWalk\n",
    "\n",
    "- **Vanilla NN:** uses features, ignores edges  \n",
    "- **DeepWalk:** uses edges, ignores features  \n",
    "\n",
    "They capture opposite halves of the graph’s information.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Combine Both?\n",
    "\n",
    "Real tasks require **attributes + structure**.  \n",
    "Example: paper classification needs both the paper’s text (features) and its citation links (structure).\n",
    "\n",
    "---\n",
    "\n",
    "## Integrated Methods: GNNs\n",
    "\n",
    "Graph Neural Networks combine both worlds:  \n",
    "each node **aggregates features from its neighbors** and mixes them with its own.  \n",
    "The result represents both **what the node is** and **who it is connected to**.\n",
    "\n",
    "---\n",
    "\n",
    "## One-Sentence Summary\n",
    "\n",
    "Vanilla NNs see only features, DeepWalk sees only structure, and GNNs combine both for complete graph understanding.\n",
    "\n",
    "</div>\n",
    "\n",
    "<center><img src=\"GNN_bootcamp-main/images/Two-extremes.png\" width=500></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2517c5",
   "metadata": {},
   "source": [
    "<!-- Option A: HTML box + LaTeX (works in Jupyter/MathJax-enabled viewers) -->\n",
    "<div style=\"background:#f7f7f7; padding:18px; border-radius:10px; line-height:1.45;\">\n",
    "\n",
    "## What we start with\n",
    "- A graph: nodes connected by edges (e.g., A—B, A—C).  \n",
    "- Feature matrix $X \\in \\mathbb{R}^{n\\times d}$.  \n",
    "  Example ($n=3, d=2$):\n",
    "\n",
    "$$X = \\begin{bmatrix} 1.0 & 0.5\\\\[4pt] 0.2 & 0.1\\\\[4pt] 0.7 & 0.8 \\end{bmatrix}$$\n",
    "\n",
    "---\n",
    "<center><img src=\"GNN_bootcamp-main/images/gcn_intro.png\" width=\"300\"></center>\n",
    "\n",
    "## Vanilla neural layer (baseline)\n",
    "- Formula:\n",
    "$$H^{(1)} = \\sigma(XW)$$\n",
    "- Meaning: apply a regular NN layer to each node independently (edges ignored).\n",
    "\n",
    "**Example:** Node A update = $\\sigma([1.0,\\,0.5]W)$ — no influence from B,C.\n",
    "\n",
    "---\n",
    "\n",
    "## Bring in graph structure (adjacency)\n",
    "- Let A be the adjacency matrix, where $A_{ij}=1$ if $i$ connects to $j$.\n",
    "- New idea:\n",
    "$$H^{(1)} = \\sigma(A X W)$$\n",
    "- Effect: each row becomes the sum of neighbors' features (node does not include its own feature yet).\n",
    "\n",
    "**Example:** If A connects to B,C, then row for A in AX = $x_B + x_C$.\n",
    "\n",
    "\n",
    "<center><img src=\"GNN_bootcamp-main/images/GCN_naive.png\" width=\"500\"></center>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Add self-loops (include node itself)\n",
    "- Define:\n",
    "$$\\tilde{A} = A + I$$\n",
    "- Now $\\tilde{A}X$ aggregates self + neighbors.\n",
    "\n",
    "**Example:** row(A) = $x_A + x_B + x_C$.\n",
    "\n",
    "---\n",
    "\n",
    "## Degree imbalance problem\n",
    "- Nodes with many neighbors sum many vectors → large magnitude; nodes with few neighbors get small magnitude — this biases training.\n",
    "\n",
    "---\n",
    "\n",
    "## Normalize with degree\n",
    "- Degree matrix:\n",
    "$$\\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}$$\n",
    "- Symmetric normalization:\n",
    "$$\\tilde{A}_{\\text{norm}} = \\tilde{D}^{-1/2}\\,\\tilde{A}\\,\\tilde{D}^{-1/2}$$\n",
    "This keeps the operator symmetric and stabilizes training.\n",
    "\n",
    "---\n",
    "\n",
    "## Final GCN propagation rule\n",
    "$$H^{(1)} = \\sigma\\!\\left(\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2} X W\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "## Mini numerical example (corrected)\n",
    "Node feature vectors:\n",
    "$$x_A=[1,1],\\quad x_B=[2,0],\\quad x_C=[0,3]$$\n",
    "\n",
    "Adjacency (no self-loops initially):\n",
    "$$A=\\begin{bmatrix}0&1&1\\\\[4pt]1&0&0\\\\[4pt]1&0&0\\end{bmatrix} \\quad\\Rightarrow\\quad \\tilde{A}=A+I=\\begin{bmatrix}1&1&1\\\\[4pt]1&1&0\\\\[4pt]1&0&1\\end{bmatrix}$$\n",
    "\n",
    "Degrees:\n",
    "$$\\tilde{D}=\\mathrm{diag}(3,\\;2,\\;2)$$\n",
    "\n",
    "Aggregate (row sums):\n",
    "$$\\tilde{A}X= \\begin{bmatrix} x_A+x_B+x_C\\\\[4pt] x_B+x_A\\\\[4pt] x_C+x_A \\end{bmatrix} = \\begin{bmatrix} 3 & 4\\\\[4pt] 3 & 1\\\\[4pt] 1 & 4 \\end{bmatrix}$$\n",
    "\n",
    "Apply symmetric normalization exactly:\n",
    "$$\\tilde{D}^{-1/2}= \\mathrm{diag}\\!\\left(\\tfrac{1}{\\sqrt{3}},\\tfrac{1}{\\sqrt{2}},\\tfrac{1}{\\sqrt{2}}\\right)$$\n",
    "So\n",
    "$$\\tilde{A}_{\\text{norm}}=\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2} = \\begin{bmatrix} \\tfrac{1}{3} & \\tfrac{1}{\\sqrt{6}} & \\tfrac{1}{\\sqrt{6}}\\\\[6pt] \\tfrac{1}{\\sqrt{6}} & \\tfrac{1}{2} & 0\\\\[6pt] \\tfrac{1}{\\sqrt{6}} & 0 & \\tfrac{1}{2} \\end{bmatrix}$$\n",
    "\n",
    "Normalized node features $=\\tilde{A}_{\\text{norm}} X$. Numerically:\n",
    "$$\\tilde{A}_{\\text{norm}}X \\approx \\begin{bmatrix} (1/3)\\cdot[1,1] + (1/\\sqrt6)\\cdot[2,0] + (1/\\sqrt6)\\cdot[0,3]\\\\[4pt] (1/\\sqrt6)\\cdot[1,1] + (1/2)\\cdot[2,0]\\\\[4pt] (1/\\sqrt6)\\cdot[1,1] + (1/2)\\cdot[0,3] \\end{bmatrix} \\approx \\begin{bmatrix} [1.149,\\;1.558]\\\\[4pt] [1.408,\\;0.408]\\\\[4pt] [0.408,\\;1.908] \\end{bmatrix}$$\n",
    "\n",
    "Then apply $W$ and $\\sigma$ to get the final layer output.\n",
    "\n",
    "---\n",
    "\n",
    "## Ultra-compact checklist\n",
    "- $XW$: vanilla NN, ignores neighbors.  \n",
    "- $A X W$: neighbors only, ignores self.  \n",
    "- $(A+I) X W$: includes self, unnormalized (high-degree inflation).  \n",
    "- $\\tilde{D}^{-1/2}(A+I)\\tilde{D}^{-1/2}$: symmetric normalization (GCN).  \n",
    "- Final: $H=\\sigma(\\tilde{D}^{-1/2}(A+I)\\tilde{D}^{-1/2} X W)$.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c8fcc",
   "metadata": {},
   "source": [
    "<!-- GCN: Formal explanation (KaTeX-safe) -->\n",
    "<div style=\"background:#f7f7f7; padding:18px; border-radius:10px; line-height:1.45;\">\n",
    "\n",
    "## Clear, step-by-step explanation: “GCN — more formal”\n",
    "\n",
    "I’ll structure it as:\n",
    "1. What the objects mean  \n",
    "2. What message passing means  \n",
    "3. What the GCN formula is doing  \n",
    "4. How convolution appears in graphs  \n",
    "5. Intuition tie-back to your A–B–C–D graph\n",
    "\n",
    "---\n",
    "\n",
    "## 1. What the objects mean\n",
    "\n",
    "- $H^{(0)} = X$ — initial node features (e.g., vectors for A, B, C, D).  \n",
    "- $H^{(l-1)}$ — feature matrix from the previous layer.  \n",
    "- $W$ — learned weight matrix (like in standard neural nets).  \n",
    "- $A$ — adjacency matrix (which nodes are connected).  \n",
    "- $A+I$ — adjacency with self-loops (each node includes itself).  \n",
    "- $D$ — degree matrix (diagonal; counts neighbors).  \n",
    "- $D^{-1/2}(A+I)D^{-1/2}$ — symmetric normalized adjacency (balances nodes with different degrees).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Message Passing: the core idea\n",
    "\n",
    "Message passing steps:\n",
    "- Each node sends its current feature vector to neighbors.\n",
    "- Each node receives neighbors' vectors.\n",
    "- Each node aggregates received vectors (sum or average).\n",
    "- Apply a learned linear transform $W$.\n",
    "- Apply a nonlinearity $\\sigma$ (e.g., ReLU).\n",
    "\n",
    "<center><img src=\"GNN_bootcamp-main/images/GCN_overview.png\" width=\"500\"></center>\n",
    "\n",
    "Intuition: a node updates by “looking around itself.”  \n",
    "Example for node A:\n",
    "- neighbors: B, C  \n",
    "- receives x_B and x_C, aggregates them (plus optionally x_A if self-looped), then applies $W$ and $\\sigma$.\n",
    "\n",
    "<center><img src=\"GNN_bootcamp-main/images/message_passing.gif\"></center>\n",
    "\n",
    "---\n",
    "\n",
    "## 3. The GCN formula explained slowly\n",
    "\n",
    "A single GCN layer:\n",
    "$$H^{(l)} = \\sigma\\!\\big( \\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}\\,H^{(l-1)}\\,W \\big), \\qquad \\tilde{A}=A+I$$\n",
    "\n",
    "Step-by-step:\n",
    "1. Add self-loops: $\\tilde{A}=A+I$. Each node includes its own features in the aggregate.\n",
    "2. Compute degrees: $\\tilde{D}_{ii}=\\sum_j \\tilde{A}_{ij}$.\n",
    "3. Symmetric normalization: $\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$ — prevents high-degree nodes from dominating.\n",
    "4. Aggregate: $\\tilde{A}_{\\text{norm}}\\,H^{(l-1)}$ — each row becomes the normalized blend of a node’s neighborhood features.\n",
    "5. Linear transform: $(\\cdots)W$ — mixes feature dimensions (same as a dense layer).\n",
    "6. Nonlinearity: $\\sigma(\\cdot)$ — introduces nonlinearity and enables deeper stacking.\n",
    "\n",
    "Notes:\n",
    "- Normalization makes contributions proportional rather than additive with degree.\n",
    "- Using the symmetric form keeps the operator self-adjoint (useful for stability).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Where is the “convolution”?\n",
    "\n",
    "Analogy with CNNs:\n",
    "- CNN: a filter slides over a grid, collects a pixel and nearby pixels, applies a shared kernel.\n",
    "- GCN: a filter sits on each node, collects that node and its neighbors, applies a shared linear operator $W$.\n",
    "\n",
    "Convolution roles:\n",
    "- Neighborhood extractor: $\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$.\n",
    "- Shared filter: $W$.\n",
    "Thus GCN implements a graph-aware, parameter-shared local aggregation — the graph version of convolution.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Tie-back to the A–B–C–D graph\n",
    "\n",
    "Using your graph:\n",
    "- A receives messages from A, B, C (if self-loops included).  \n",
    "- B receives A, B, C, D.  \n",
    "- C receives A, B, C.  \n",
    "- D receives B, D.\n",
    "\n",
    "After normalization:\n",
    "- Each neighbor group is averaged/weighted so no single node’s degree overpowers others.\n",
    "- Applying $W$ changes feature dimensionality and abstracts features.\n",
    "- Stack layers to increase receptive field:\n",
    "  - 1 layer → 1-hop neighbors,\n",
    "  - 2 layers → 2-hop neighbors,\n",
    "  - etc.\n",
    "\n",
    "GCNs are powerful because repeated local aggregations plus learned transforms let nodes encode progressively larger graph context.\n",
    "\n",
    "---\n",
    "\n",
    "## Okay... but where is the convolution?\n",
    "\n",
    "In fact, a Graph Convolutional Neural Network (GCN) performs operations similar to those in Convolutional Neural Networks (CNNs) used for images. In a GCN, we can think of the message passing process as moving a filter (or kernel) over each node in the graph. When the filter is on a node, it gathers and combines data from nearby nodes to create the output for that node.\"\n",
    "\n",
    "<img src=\"GNN_bootcamp-main/images/CNN_filter.png\" style=\"width: 45%; display: inline-block;\">\n",
    "<img src=\"GNN_bootcamp-main/images/GCN_filter.png\" style=\"width: 45%; display: inline-block;\">\n",
    "<center>convolution in CNN vs convolution in GCN</center>\n",
    "Very similar to CNN, for GCNs, a filter is passed over each node and the values of the **neighboring nodes** are combined to form the output value at the next layer.\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4e137c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Cora: N=2708, features=1433, classes=7\n",
      "A (sample rows):\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "A + I (sample rows):\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "Degrees (first 10): [4. 4. 6. 2. 6. 4. 5. 2. 4. 3.]\n",
      "\n",
      "A_norm (sample rows):\n",
      "[[0.25       0.         0.         0.         0.         0.        ]\n",
      " [0.         0.25       0.20412414 0.         0.         0.        ]\n",
      " [0.         0.20412414 0.16666666 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.49999997 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.16666666 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.25      ]]\n",
      "\n",
      "Symmetry check max |A_norm - A_norm^T| = 0.0\n",
      "Epoch 001 | Loss 1.9471 | Train 0.2857 | Val 0.1120 | Test 0.1350\n",
      "Epoch 020 | Loss 0.6076 | Train 0.9857 | Val 0.7500 | Test 0.7520\n",
      "Epoch 040 | Loss 0.1024 | Train 1.0000 | Val 0.7800 | Test 0.7810\n",
      "Epoch 060 | Loss 0.0770 | Train 1.0000 | Val 0.7740 | Test 0.7770\n",
      "Epoch 080 | Loss 0.0497 | Train 1.0000 | Val 0.7680 | Test 0.7920\n",
      "Epoch 100 | Loss 0.0785 | Train 1.0000 | Val 0.7720 | Test 0.7990\n",
      "Epoch 120 | Loss 0.0518 | Train 1.0000 | Val 0.7600 | Test 0.7960\n",
      "Epoch 140 | Loss 0.0420 | Train 1.0000 | Val 0.7680 | Test 0.7960\n",
      "Epoch 160 | Loss 0.0393 | Train 1.0000 | Val 0.7620 | Test 0.7990\n",
      "Epoch 180 | Loss 0.0455 | Train 1.0000 | Val 0.7660 | Test 0.8000\n",
      "Epoch 200 | Loss 0.0419 | Train 1.0000 | Val 0.7720 | Test 0.8030\n",
      "Best val 0.7860 @ epoch 26, Test at that val = 0.7800\n",
      "\n",
      "(A_norm @ X) sample rows (first 6):\n",
      "[[0.         0.         0.         ... 0.         0.2236068  0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.13608277 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.2236068  0.        ]]\n",
      "\n",
      "Logistic regression on hidden GCN embeddings (test) accuracy: 0.7780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\linear_model\\_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Manual GCN implementation for Cora that follows:\n",
    "# H^{(l)} = sigma( D^{-1/2} (A+I) D^{-1/2} H^{(l-1)} W )\n",
    "# - Computes A, A+I, D, D^{-1/2} (A+I) D^{-1/2}\n",
    "# - Uses these matrices directly (manual propagation) so it's explicit and educational\n",
    "# - Trains a 2-layer GCN end-to-end and evaluates logistic regression on hidden embeddings\n",
    "# Note: requires torch and torch_geometric\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# torch_geometric utilities\n",
    "try:\n",
    "    from torch_geometric.datasets import Planetoid\n",
    "    from torch_geometric.utils import to_dense_adj, add_self_loops\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Please install torch_geometric. See https://pytorch-geometric.readthedocs.io/\") from e\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# Load Cora\n",
    "# ---------------------------\n",
    "dataset = Planetoid(root=\"data/Planetoid\", name=\"Cora\")\n",
    "data = dataset[0].to(device)\n",
    "N = data.num_nodes\n",
    "F_in = data.num_node_features\n",
    "num_classes = dataset.num_classes\n",
    "print(f\"Cora: N={N}, features={F_in}, classes={num_classes}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Build dense adjacency A (N x N)\n",
    "# ---------------------------\n",
    "# to_dense_adj returns shape [batch, N, N], batch=1 for single graph\n",
    "A_dense = to_dense_adj(data.edge_index, max_num_nodes=N)[0].to(device)  # tensor float (0/1)\n",
    "A_dense = A_dense.to(dtype=torch.float32)\n",
    "\n",
    "# Print a small part\n",
    "print(\"A (sample rows):\")\n",
    "print(A_dense[:6, :6].cpu().numpy())\n",
    "\n",
    "# ---------------------------\n",
    "# Add self-loops: A_hat = A + I\n",
    "# ---------------------------\n",
    "I = torch.eye(N, device=device)\n",
    "A_hat = A_dense + I\n",
    "\n",
    "print(\"\\nA + I (sample rows):\")\n",
    "print(A_hat[:6, :6].cpu().numpy())\n",
    "\n",
    "# ---------------------------\n",
    "# Degree matrix D_hat and D_hat^{-1/2}\n",
    "# ---------------------------\n",
    "deg = A_hat.sum(dim=1)            # shape [N]\n",
    "# Avoid division by zero (shouldn't happen after self-loops)\n",
    "deg_inv_sqrt = torch.pow(deg, -0.5)\n",
    "deg_inv_sqrt[torch.isinf(deg_inv_sqrt)] = 0.0\n",
    "D_inv_sqrt = torch.diag(deg_inv_sqrt)\n",
    "\n",
    "print(\"\\nDegrees (first 10):\", deg[:10].cpu().numpy())\n",
    "\n",
    "# ---------------------------\n",
    "# Symmetric normalized adjacency: A_norm = D^{-1/2} * A_hat * D^{-1/2}\n",
    "# ---------------------------\n",
    "A_norm = D_inv_sqrt @ A_hat @ D_inv_sqrt\n",
    "print(\"\\nA_norm (sample rows):\")\n",
    "print(A_norm[:6, :6].cpu().numpy())\n",
    "\n",
    "# ---------------------------\n",
    "# Quick check: A_norm should be symmetric (numerically)\n",
    "# ---------------------------\n",
    "sym_diff = (A_norm - A_norm.t()).abs().max().item()\n",
    "print(\"\\nSymmetry check max |A_norm - A_norm^T| =\", sym_diff)\n",
    "\n",
    "# ---------------------------\n",
    "# Manual GCN model (explicit matrix multiplications)\n",
    "# ---------------------------\n",
    "class ManualGCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats, A_norm):\n",
    "        super().__init__()\n",
    "        # store normalized adjacency as buffer (not a parameter)\n",
    "        self.register_buffer(\"A_norm\", A_norm)  # shape [N, N]\n",
    "        self.W1 = nn.Parameter(torch.randn(in_feats, hidden_feats) * 0.1)\n",
    "        self.W2 = nn.Parameter(torch.randn(hidden_feats, out_feats) * 0.1)\n",
    "        self.dropout = 0.5\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X: [N, F_in]\n",
    "        # 1st layer: H1 = sigma( A_norm @ X @ W1 )\n",
    "        H1 = self.A_norm @ X @ self.W1\n",
    "        H1 = F.relu(H1)\n",
    "        H1 = F.dropout(H1, p=self.dropout, training=self.training)\n",
    "        # 2nd layer: logits = A_norm @ H1 @ W2\n",
    "        logits = self.A_norm @ H1 @ self.W2\n",
    "        return logits, H1  # return logits and hidden embeddings\n",
    "\n",
    "# instantiate model\n",
    "hidden_dim = 16\n",
    "model = ManualGCN(in_feats=F_in, hidden_feats=hidden_dim, out_feats=num_classes, A_norm=A_norm).to(device)\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters()], lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# ---------------------------\n",
    "# Training loop\n",
    "# ---------------------------\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    logits, _ = model(data.x)\n",
    "    loss = F.cross_entropy(logits[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    logits, H1 = model(data.x)\n",
    "    preds = logits.argmax(dim=1)\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = preds[mask].eq(data.y[mask]).sum().item()\n",
    "        acc = correct / int(mask.sum().item())\n",
    "        accs.append(acc)\n",
    "    return tuple(accs), logits.cpu().numpy(), H1.cpu().numpy()\n",
    "\n",
    "best_val = 0.0\n",
    "best_test_at_best_val = 0.0\n",
    "best_epoch = 0\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train_epoch()\n",
    "    (train_acc, val_acc, test_acc), logits_np, H1_np = evaluate()\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc\n",
    "        best_test_at_best_val = test_acc\n",
    "        best_epoch = epoch\n",
    "    if epoch == 1 or epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Loss {loss:.4f} | Train {train_acc:.4f} | Val {val_acc:.4f} | Test {test_acc:.4f}\")\n",
    "\n",
    "print(f\"Best val {best_val:.4f} @ epoch {best_epoch}, Test at that val = {best_test_at_best_val:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Inspect A_norm @ X (structure-aware features BEFORE applying W)\n",
    "# ---------------------------\n",
    "with torch.no_grad():\n",
    "    A_norm_X = A_norm @ data.x\n",
    "print(\"\\n(A_norm @ X) sample rows (first 6):\")\n",
    "print(A_norm_X[:6].cpu().numpy())\n",
    "\n",
    "# ---------------------------\n",
    "# Use hidden embeddings H1 for logistic regression\n",
    "# ---------------------------\n",
    "train_idx = np.where(data.train_mask.cpu().numpy())[0]\n",
    "test_idx = np.where(data.test_mask.cpu().numpy())[0]\n",
    "y = data.y.cpu().numpy()\n",
    "\n",
    "X_train = H1_np[train_idx]\n",
    "y_train = y[train_idx]\n",
    "X_test = H1_np[test_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, multi_class=\"ovr\")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "acc_lr = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nLogistic regression on hidden GCN embeddings (test) accuracy: {acc_lr:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41300b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
